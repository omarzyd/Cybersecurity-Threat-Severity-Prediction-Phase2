# -*- coding: utf-8 -*-
"""ML Project Phase 2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ib6LwkVz0lOpH3KqtEInz8D72OCzrpXE
"""

import pandas as pd
df=pd.read_csv('/content/train.csv')

df.info()

df.describe()

df.duplicated().sum()

df.drop_duplicates(inplace=True)

import matplotlib.pyplot as plt
import seaborn as sns
numerical_data = df.select_dtypes(include=['int64','float64'])
for column in numerical_data.columns:
    fig,ax = plt.subplots(figsize=(4,6))
    sns.boxplot(x=df[column])
    plt.title(column)
    plt.show()

columnsName = df.select_dtypes(include=['int64','float64']).columns

numerical_data = pd.DataFrame(numerical_data,columns=columnsName)
numerical_data.hist(bins=30, figsize=(20, 15))
plt.suptitle("Histograms of Numerical Features")
plt.show()

for column in df.select_dtypes(include=['object']).columns:
    print(f"Unique values in '{column}':")
    print(df[column].unique(),'\n')

df_unscaled=df.copy()

from sklearn.preprocessing import PowerTransformer
pt = PowerTransformer(method='yeo-johnson', standardize=True)
numerical_cols = numerical_data.columns
df[numerical_cols] = pt.fit_transform(df[numerical_cols])

from sklearn.preprocessing import LabelEncoder
encoder=LabelEncoder()
df.loc[:,'label']=encoder.fit_transform(df['label'])
df_unscaled.loc[:,'label']=encoder.fit_transform(df_unscaled['label'])

df_unscaled.corr()

sns.heatmap(df_unscaled.corr(),cmap='coolwarm')
plt.title('Correlation between Columns')
plt.show()

missing_values=df.isnull().sum()
missing_values

df.dropna(inplace=True)

X=df.drop('label',axis=1)
Y=df['label'].astype(int)

X_unscaled=df_unscaled.drop('label',axis=1)
Y_unscaled=df_unscaled['label'].astype(int)

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.2,random_state=42)

x_unscaled_train,x_unscaled_test,y_unscaled_train,y_unscaled_test=train_test_split(X_unscaled,Y_unscaled,test_size=0.2,random_state=42)

from imblearn.over_sampling import SMOTE
smote = SMOTE(random_state=42)
x_train, y_train = smote.fit_resample(x_train, y_train)

"""Decision Trees

Default Depth
"""

from sklearn.model_selection import GridSearchCV
from sklearn.tree import DecisionTreeClassifier,export_text,plot_tree
DecisionTreeModel = DecisionTreeClassifier(random_state=42)
DecisionTreeModel.fit(x_train,y_train)
DecisionTree_y_pred= DecisionTreeModel.predict(x_test)
txtRep = export_text(DecisionTreeModel)
inputVariable = df.columns
plot_tree(DecisionTreeModel , feature_names= inputVariable , filled=True)

from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score,roc_auc_score,classification_report, mean_absolute_error, mean_squared_error, r2_score
print(f" Accuracy Score = {accuracy_score(y_test, DecisionTree_y_pred)}")
print(f" Precision Score = {precision_score(y_test, DecisionTree_y_pred, average='weighted')}")
print(f" Recall Score = {recall_score(y_test, DecisionTree_y_pred, average='weighted')}")
print(f" F1 Score = {f1_score(y_test, DecisionTree_y_pred, average='weighted')}")
print(f" Confusion Matrix = {confusion_matrix(y_test, DecisionTree_y_pred)}")
print(f"Classification Report = {classification_report(y_test,DecisionTree_y_pred)}")

"""Depth=2"""

TreeModel_2=DecisionTreeClassifier(max_depth=2 ,random_state=42)
TreeModel_2.fit(x_train,y_train)
Tree_y_pred_2=TreeModel_2.predict(x_test)
txtRep=export_text(TreeModel_2)
inputVariable = df.columns
plot_tree(TreeModel_2, feature_names= inputVariable , filled=True)

print(f" Accuracy Score = {accuracy_score(y_test, Tree_y_pred_2)}")
print(f" Precision Score = {precision_score(y_test, Tree_y_pred_2, average='weighted')}")
print(f" Recall Score = {recall_score(y_test, Tree_y_pred_2, average='weighted')}")
print(f" F1 Score = {f1_score(y_test, Tree_y_pred_2, average='weighted')}")
print(f" Confusion Matrix = {confusion_matrix(y_test, Tree_y_pred_2)}")
print(f"Classification Report = {classification_report(y_test,Tree_y_pred_2)}")

"""Hyper Parameter Tuning (GridSearch)"""

param_grid = {'max_depth': range(1, 31)}
TreeGrid = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5)
TreeGrid.fit(x_train, y_train)

best_params = TreeGrid.best_params_
BestTree= TreeGrid.best_estimator_

print(f" Accuracy Score = {accuracy_score(y_test, Tree_y_pred)}")
print(f" Precision Score = {precision_score(y_test, Tree_y_pred, average='weighted')}")
print(f" Recall Score = {recall_score(y_test, Tree_y_pred, average='weighted')}")
print(f" F1 Score = {f1_score(y_test, Tree_y_pred, average='weighted')}")
print(f" Confusion Matrix = {confusion_matrix(y_test, Tree_y_pred)}")
print(f"Classification Report = {classification_report(y_test,Tree_y_pred)}")

"""KNN

Default Nearest Neighbor
"""

from sklearn.neighbors import KNeighborsClassifier
KNNModel= KNeighborsClassifier()
KNNModel.fit(x_train,y_train)
KNN_y_pred= KNNModel.predict(x_test)

print(f" Accuracy Score = {accuracy_score(y_test, KNN_y_pred)}")
print(f" Precision Score = {precision_score(y_test, KNN_y_pred, average='weighted')}")
print(f" Recall Score = {recall_score(y_test, KNN_y_pred, average='weighted')}")
print(f" F1 Score = {f1_score(y_test, KNN_y_pred, average='weighted')}")
print(f"Confusion Matrix = {confusion_matrix(y_test, y_pred_knn)}")
print(f"Classification Report = {classification_report(y_test,KNN_y_pred)}")

"""Trying Random N-neigbors"""

highest_accuracies=0
best_k=0
for k in range(1,25,3):
    knnmodel=KNeighborsClassifier(n_neighbors=k)
    knnmodel.fit(x_train, y_train)
    knn_y_pred = knnmodel.predict(x_test)
    accuracy= accuracy_score(y_test, knn_y_pred)
    if accuracy > highest_accuracies:
        highest_accuracies = accuracy
        best_k = k
    print(f"K= {k}, Acurracy= {accuracy}")

"""Best n_neighbor"""

BestKNN= KNeighborsClassifier(n_neigbor=best_k)
BestKNN.fit(x_train,y_train)
bestKNN_y_pred= KNNModel.predict(x_test)

print(f" Accuracy Score = {accuracy_score(y_test, bestKNN_y_pred)}")
print(f" Precision Score = {precision_score(y_test, bestKNN_y_pred, average='weighted')}")
print(f" Recall Score = {recall_score(y_test, bestKNN_y_pred, average='weighted')}")
print(f" F1 Score = {f1_score(y_test, bestKNN_y_pred, average='weighted')}")
print(f"Confusion Matrix = {confusion_matrix(y_test, bestKNN_y_pred)}")
print(f"Classification Report = {classification_report(y_test,bestKNN_y_pred)}")

"""Random Forest"""

from sklearn.ensemble import RandomForestClassifier
RF_model= RandomForestClassifier(random_state=42)
RF_model.fit(x_train, y_train)
RF_pred= RF_model.predict(x_test)

print(f" Accuracy Score = {accuracy_score(y_test, RF_pred)}")
print(f" Precision Score = {precision_score(y_test, RF_pred, average='weighted')}")
print(f" Recall Score = {recall_score(y_test, RF_pred, average='weighted')}")
print(f" F1 Score = {f1_score(y_test, RF_pred, average='weighted')}")
print(f" Confusion Matrix = {confusion_matrix(y_test, RF_pred)}")
print(f"Classification Report = {classification_report(y_test,RF_pred)}")

"""Random Forest Hyper Parameter Tuning (GridSearch)"""

from sklearn.model_selection import GridSearchCV
param_grid={'n_estimators':[100,200],'max_depth':[10,20,None],'min_samples_split':[2,5]}
grid_search=GridSearchCV(RandomForestClassifier(random_state=42),param_grid,cv=3,scoring='accuracy',n_jobs=-1)
grid_search.fit(x_train,y_train)
best_params=grid_search.best_params_
BestRandomForestGS=grid_search.best_estimator_
BestRandomForestGS_pred=BestRandomForestGS.predict(x_test)

print(f" Accuracy Score = {accuracy_score(y_test, BestRandomForestGS_pred)}")
print(f" Precision Score = {precision_score(y_test, BestRandomForestGS_pred, average='weighted')}")
print(f" Recall Score = {recall_score(y_test, BestRandomForestGS_pred, average='weighted')}")
print(f" F1 Score = {f1_score(y_test, BestRandomForestGS_pred, average='weighted')}")
print(f" Confusion Matrix = {confusion_matrix(y_test, BestRandomForestGS_pred)}")
print(f"Classification Report = {classification_report(y_test,BestRandomForestGS_pred)}")

"""Random Forest Hyper Parameter Tuning (RandomSearch)

"""

from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint
from sklearn.ensemble import RandomForestClassifier
distributions = {
    'n_estimators': randint(100, 300),
    'max_depth': randint(3, 15),
    'min_samples_split': randint(2, 10),
    'min_samples_leaf': randint(1, 10)
}
rf = RandomForestClassifier(random_state=42)
RandomSearchRF = RandomizedSearchCV(RF_model, distributions, random_state=0)
random_search = RandomSearchRF.fit(x_train, y_train)
best_modelRS = random_search.best_params_
BestRandomForestRS = RandomSearchRF.best_estimator_
BestRandomForestRS_pred= BestRandomForestRS.predict(x_test)

print(f" Accuracy Score = {accuracy_score(y_test, BestRandomForestRS_pred)}")
print(f" Precision Score = {precision_score(y_test, BestRandomForestRS_pred, average='weighted')}")
print(f" Recall Score = {recall_score(y_test, BestRandomForestRS_pred, average='weighted')}")
print(f" F1 Score = {f1_score(y_test, BestRandomForestRS_pred, average='weighted')}")
print(f" Confusion Matrix = {confusion_matrix(y_test, BestRandomForestRS_pred)}")
print(f"Classification Report = {classification_report(y_test,BestRandomForestRS_pred)}")

"""XGBoost"""

from xgboost import XGBClassifier
xgb_model = XGBClassifier(eval_metric='mlogloss', random_state=42)
xgb_model.fit(x_unscaled_train, y_unscaled_train)
xgb_y_pred = xgb_model.predict(x_unscaled_test)

print(f" Accuracy Score = {accuracy_score(y_unscaled_test, xgb_y_pred)}")
print(f" Confusion Matrix = {confusion_matrix(y_unscaled_test, xgb_y_pred)}")
print(f" Precision Score = {precision_score(y_unscaled_test, xgb_y_pred, average='weighted')}")
print(f" Recall Score = {recall_score(y_unscaled_test, xgb_y_pred, average='weighted')}")
print(f" F1 Score = {f1_score(y_unscaled_test, xgb_y_pred, average='weighted')}")
print(f"Classification Report = {classification_report(y_unscaled_test,xgb_y_pred)}")

"""Hyper Parameter Tuning (GridSearch)"""

param_grid = {'max_depth': [3, 5, 7], 'learning_rate': [0.01, 0.1, 0.2], 'n_estimators': [100, 200], 'subsample': [0.8], 'colsample_bytree': [0.8]}
xgb = XGBClassifier(eval_metric='mlogloss',tree_method='hist', random_state=42)
xgb_grid_search= GridSearchCV(xgb, param_grid, scoring='accuracy', cv=3)
xgb_grid_search.fit(x_unscaled_train, y_unscaled_train)
xgb_best_grid_search = xgb_grid_search.best_estimator_
best_xgb_y_pred = xgb_best_grid_search.predict(x_unscaled_test)

print(f" Accuracy Score = {accuracy_score(y_unscaled_test, best_xgb_y_pred)}")
print(f" Precision Score = {precision_score(y_unscaled_test, best_xgb_y_pred, average='weighted')}")
print(f" Recall Score = {recall_score(y_unscaled_test, best_xgb_y_pred, average='weighted')}")
print(f" F1 Score = {f1_score(y_unscaled_test, best_xgb_y_pred, average='weighted')}")
print(f" Confusion Matrix = {confusion_matrix(y_unscaled_test, best_xgb_y_pred)}")
print(f"Classification Report = {classification_report(y_unscaled_test,best_xgb_y_pred)}")

"""Hyper Parameter Tuning (RandomSearch)"""

from scipy.stats import randint, uniform
from sklearn.model_selection import RandomizedSearchCV
xgb = XGBClassifier(eval_metric='mlogloss',tree_method='hist', random_state=42)
param_dist = {'n_estimators': randint(100, 300), 'learning_rate': uniform(0.01, 0.2), 'max_depth': randint(4, 10), 'subsample': uniform(0.7, 0.3),
              'colsample_bytree': uniform(0.7, 0.3),'gamma': uniform(0, 5),'min_child_weight': randint(1, 10)}

xgb_random_search = RandomizedSearchCV(xgb, param_distributions=param_dist, n_iter=25, scoring='accuracy', n_jobs=2, cv=3,random_state=42)
xgb_random_search.fit(x_unscaled_train, y_unscaled_train)
xgb_best_random_search= xgb_random_search.best_estimator_
random_search_xgb_y_pred = xgb_best_random_search.predict(x_unscaled_test)

print(f" Accuracy Score = {accuracy_score(y_unscaled_test, random_search_xgb_y_pred)}")
print(f" Precision Score = {precision_score(y_unscaled_test, random_search_xgb_y_pred, average='weighted')}")
print(f" Recall Score = {recall_score(y_unscaled_test, random_search_xgb_y_pred, average='weighted')}")
print(f" F1 Score = {f1_score(y_unscaled_test, random_search_xgb_y_pred, average='weighted')}")
print(f" Confusion Matrix = {confusion_matrix(y_unscaled_test, random_search_xgb_y_pred)}")
print(f"Classification Report = {classification_report(y_unscaled_test,random_search_xgb_y_pred)}")

"""Hyper Parameter Tuning (Optuna)

"""

pip install optuna

import optuna
from sklearn.model_selection import cross_val_score
import numpy as np
from xgboost import XGBClassifier
y_unique = len(np.unique(y_train))
study = optuna.create_study(direction='maximize')
for trial in range(30):
    trialModel = study.ask()
    parametres = {
        'objective': 'multi:softprob',
        'num_class': y_unique,
        'eval_metric': 'mlogloss',
        'tree_method': 'hist',
        'random_state': 42,
        'n_estimators': trialModel.suggest_int('n_estimators', 100, 300),
        'learning_rate': trialModel.suggest_float('learning_rate', 0.01, 0.2),
        'max_depth': trialModel.suggest_int('max_depth', 4, 12),
        'subsample': trialModel.suggest_float('subsample', 0.6, 1.0),
        'colsample_bytree': trialModel.suggest_float('colsample_bytree', 0.6, 1.0),
        'gamma': trialModel.suggest_float('gamma', 0, 5),
        'min_child_weight': trialModel.suggest_int('min_child_weight', 1, 10)
    }

    model = XGBClassifier(**parametres)
    score = cross_val_score(model, x_unscaled_train, y_train, cv=3, scoring='accuracy', n_jobs=-1).mean()
    study.tell(trialModel, score)_unscaled
bestParams = study.best_trial.parametres
bestParams.update({
    'objective': 'multi:softprob',
    'num_class': y_unique,
    'eval_metric': 'mlogloss',
    'tree_method': 'hist',
    'random_state': 42
})
finalModel = XGBClassifier(**bestParams)
finalModel.fit(x_unscaled_train, y_unscaled_train)
XGBoptuna_pred = finalModel.predict(x_unscaled_test)

print(f" Accuracy Score = {accuracy_score(y_unscaled_test, XGBoptuna_pred)}")
print(f" Precision Score = {precision_score(y_unscaled_test, XGBoptuna_pred, average='weighted')}")
print(f" Recall Score = {recall_score(y_unscaled_test, XGBoptuna_pred, average='weighted')}")
print(f" F1 Score = {f1_score(y_unscaled_test, XGBoptuna_pred, average='weighted')}")
print(f" Confusion Matrix = {confusion_matrix(y_unscaled_test, XGBoptuna_pred)}")
print(f"Classification Report = {classification_report(y_unscaled_test,XGBoptuna_pred)}")

"""CatBoost"""

pip install catboost

from catboost import CatBoostClassifier  #ensambling method, use decision trees

catboost_model = CatBoostClassifier(
    loss_function='MultiClass',
    iterations=1000,  #number of trees
    depth=6,       #moderate to prevent overfitting
    random_seed=42,
)
catboost_model.fit(x_unscaled_train, y_unscaled_train)
catboost_y_pred = catboost_model.predict(x_unscaled_test)

print(f" Accuracy Score = {accuracy_score(y_unscaled_test, catboost_y_pred)}")
print(f" Precision Score = {precision_score(y_unscaled_test, catboost_y_pred, average='weighted')}")
print(f" Recall Score = {recall_score(y_unscaled_test, catboost_y_pred, average='weighted')}")
print(f" F1 Score = {f1_score(y_unscaled_test, catboost_y_pred, average='weighted')}")
print(f" Confusion Matrix = {confusion_matrix(y_unscaled_test, catboost_y_pred)}")
print(f"Classification Report = {classification_report(y_unscaled_test,catboost_y_pred)}")

"""LightGBM"""

from lightgbm import LGBMClassifier

lgbm_model = LGBMClassifier(
    objective='multiclass',
    num_class=len(y_unscaled_train.unique()),
    metric='multi_logloss',
    random_state=42,
)
lgbm_model.fit(x_unscaled_train, y_unscaled_train)
lgbm_y_pred = lgbm_model.predict(x_unscaled_test)

print(f" Accuracy Score = {accuracy_score(y_unscaled_test, lgbm_y_pred)}")
print(f" Precision Score = {precision_score(y_unscaled_test, lgbm_y_pred, average='weigthed')}")
print(f" Recall Score = {recall_score(y_unscaled_test, lgbm_y_pred, average='weigthed')}")
print(f" F1 Score = {f1_score(y_unscaled_test, lgbm_y_pred, average='weigthed')}")
print(f" Confusion Matrix = {confusion_matrix(y_unscaled_test, lgbm_y_pred)}")
print(f"Classification Report = {classification_report(y_unscaled_test,lgbm_y_pred)}")

"""Voting"""

from sklearn.ensemble import VotingClassifier

voting_clf = VotingClassifier(
    estimators=[
        ('Decision Tree',BestTree),
        ('Random Forest', BestRandomForestRS),
    ],

    voting='soft'
)
voting_clf.fit(x_train, y_train)
y_pred_voting = voting_clf.predict(x_test)
print("Accuracy:", accuracy_score(y_test, y_pred_voting))
print("Confusion Matrix: \n", confusion_matrix(y_test, y_pred_voting))
print("Classification Report: \n", classification_report(y_test, y_pred_voting))

"""Stacking"""

from sklearn.model_selection import StratifiedKFold
lgb = LGBMClassifier(random_state=42)
cat = CatBoostClassifier(verbose=0, random_state=42)
final_estimator = LGBMClassifier(learning_rate=0.05,n_estimators=100,num_leaves=31,andom_state=42)
stack_model = StackingClassifier(
    estimators=[
        ('xgb', xgb_best_random_search),
        ('lgb', lgbm_model),
        ('cat', catboost_model)
    ],
    final_estimator=final_estimator,
    cv=StratifiedKFold(n_splits=5),
    passthrough=True,
    n_jobs=-1
)
stack.fit(x_unscaled_train, y_unscaled_train)
y_pred_stacking= stack.predict(x_unscaled_test)
print("Accuracy:", accuracy_score(y_unscaled_test, y_pred_stacking))
print("Confusion Matrix: \n", confusion_matrix(y_unscaled_test, y_pred_stacking))
print("Classification Report: \n", classification_report(y_unscaled_test, y_pred_stacking))